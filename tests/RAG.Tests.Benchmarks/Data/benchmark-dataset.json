{
  "documents": [
    {
      "id": "doc-001",
      "text": "Machine learning is a subset of artificial intelligence that focuses on training algorithms to learn patterns from data without being explicitly programmed. It uses statistical techniques to enable computers to improve their performance on tasks through experience.",
      "source": "ml-intro.pdf"
    },
    {
      "id": "doc-002",
      "text": "Natural language processing (NLP) is a branch of AI that enables computers to understand, interpret, and generate human language. It combines computational linguistics with machine learning and deep learning to process text and speech.",
      "source": "nlp-basics.pdf"
    },
    {
      "id": "doc-003",
      "text": "Retrieval-Augmented Generation (RAG) is an architecture that enhances large language models by grounding their responses in external knowledge sources. RAG systems first retrieve relevant documents from a knowledge base, then use those documents as context for the language model to generate accurate, fact-based responses.",
      "source": "rag-overview.pdf"
    },
    {
      "id": "doc-004",
      "text": "Vector embeddings are numerical representations of text that capture semantic meaning. Dense vector representations enable similarity-based search, where documents with similar meanings have vectors that are close together in high-dimensional space.",
      "source": "embeddings-guide.pdf"
    },
    {
      "id": "doc-005",
      "text": "BM25 (Best Matching 25) is a keyword-based ranking function used in information retrieval. It ranks documents based on term frequency, document length, and inverse document frequency. BM25 is particularly effective for exact keyword matches.",
      "source": "bm25-algorithm.pdf"
    },
    {
      "id": "doc-006",
      "text": "Elasticsearch is a distributed search and analytics engine built on Apache Lucene. It provides full-text search capabilities, real-time indexing, and supports BM25 ranking. Elasticsearch is widely used for log analytics and search applications.",
      "source": "elasticsearch-docs.pdf"
    },
    {
      "id": "doc-007",
      "text": "Qdrant is a vector similarity search engine designed for high-performance semantic search. It stores and searches vector embeddings efficiently, supporting filtering, payload storage, and HNSW (Hierarchical Navigable Small World) indexing for fast approximate nearest neighbor search.",
      "source": "qdrant-intro.pdf"
    },
    {
      "id": "doc-008",
      "text": "Dense retrieval uses neural networks to encode queries and documents into dense vector representations. Unlike keyword-based methods, dense retrieval can find semantically similar documents even when they don't share exact keywords with the query.",
      "source": "dense-retrieval.pdf"
    },
    {
      "id": "doc-009",
      "text": "Hybrid search combines multiple retrieval strategies, typically BM25 keyword search and dense vector search. By leveraging both exact matching and semantic similarity, hybrid search provides better coverage and accuracy than either method alone.",
      "source": "hybrid-search.pdf"
    },
    {
      "id": "doc-010",
      "text": "Sentence transformers are neural network models that generate semantically meaningful sentence embeddings. Models like all-MiniLM-L6-v2 are optimized for semantic search and can encode text into 384-dimensional vectors.",
      "source": "sentence-transformers.pdf"
    },
    {
      "id": "doc-011",
      "text": "Document chunking is the process of splitting long documents into smaller, semantically coherent segments. Effective chunking strategies balance chunk size, overlap, and semantic boundaries to optimize retrieval accuracy.",
      "source": "chunking-strategies.pdf"
    },
    {
      "id": "doc-012",
      "text": "Reciprocal Rank Fusion (RRF) is a technique for combining ranked lists from multiple retrieval systems. It assigns scores based on the reciprocal of each document's rank, providing a simple yet effective way to merge BM25 and dense retrieval results.",
      "source": "rrf-fusion.pdf"
    },
    {
      "id": "doc-013",
      "text": "Large language models (LLMs) like GPT-4 and Claude are transformer-based neural networks trained on vast amounts of text. They can generate coherent text, answer questions, and perform various language tasks through few-shot or zero-shot learning.",
      "source": "llm-overview.pdf"
    },
    {
      "id": "doc-014",
      "text": "Precision measures the fraction of retrieved documents that are relevant to a query. High precision means most of the returned results are useful. It's calculated as: Precision = (Relevant Retrieved) / (Total Retrieved).",
      "source": "ir-metrics.pdf"
    },
    {
      "id": "doc-015",
      "text": "Recall measures the fraction of relevant documents that were successfully retrieved. High recall means the system found most of the relevant documents. It's calculated as: Recall = (Relevant Retrieved) / (Total Relevant).",
      "source": "ir-metrics.pdf"
    },
    {
      "id": "doc-016",
      "text": "Mean Reciprocal Rank (MRR) measures how quickly a search system returns the first relevant result. It's the average of the reciprocal ranks of the first relevant document for each query, with higher values indicating better performance.",
      "source": "ir-metrics.pdf"
    },
    {
      "id": "doc-017",
      "text": "Multi-tenancy in software architecture allows a single application instance to serve multiple customers (tenants). Each tenant's data is isolated, ensuring security and privacy while sharing computational resources.",
      "source": "multi-tenancy.pdf"
    },
    {
      "id": "doc-018",
      "text": "Elasticsearch uses inverted indices to enable fast full-text search. An inverted index maps terms to the documents containing them, allowing quick lookup of documents by keyword. This structure is fundamental to BM25 ranking.",
      "source": "inverted-index.pdf"
    },
    {
      "id": "doc-019",
      "text": "HNSW (Hierarchical Navigable Small World) is a graph-based algorithm for approximate nearest neighbor search in high-dimensional spaces. It builds a multi-layer graph structure that enables sub-linear search time while maintaining high recall.",
      "source": "hnsw-algorithm.pdf"
    },
    {
      "id": "doc-020",
      "text": "OpenAI's text-embedding-ada-002 model generates 1536-dimensional embeddings optimized for semantic search and clustering. It represents the second generation of Ada embeddings with improved performance and lower cost.",
      "source": "openai-embeddings.pdf"
    },
    {
      "id": "doc-021",
      "text": "TF-IDF (Term Frequency-Inverse Document Frequency) is a classical information retrieval technique that weights terms based on their frequency in a document and rarity across the corpus. Terms that appear frequently in a document but rarely in others receive high weights.",
      "source": "tfidf.pdf"
    },
    {
      "id": "doc-022",
      "text": "Cosine similarity measures the cosine of the angle between two vectors in high-dimensional space. It's commonly used to compare vector embeddings, with values ranging from -1 to 1, where 1 indicates identical vectors.",
      "source": "similarity-metrics.pdf"
    },
    {
      "id": "doc-023",
      "text": "Query expansion techniques augment the original query with additional related terms to improve retrieval coverage. Methods include pseudo-relevance feedback, synonym expansion, and query reformulation using language models.",
      "source": "query-expansion.pdf"
    },
    {
      "id": "doc-024",
      "text": "Re-ranking models take an initial set of retrieved documents and re-order them using more sophisticated (and computationally expensive) models. Cross-encoders are popular re-ranking models that jointly encode query and document pairs.",
      "source": "reranking.pdf"
    },
    {
      "id": "doc-025",
      "text": "Knowledge distillation is a technique for transferring knowledge from a large teacher model to a smaller student model. In RAG systems, distillation can create lightweight embedding models that maintain retrieval quality while reducing latency.",
      "source": "distillation.pdf"
    },
    {
      "id": "doc-026",
      "text": "Semantic search goes beyond keyword matching to understand the meaning and intent behind queries. It uses natural language understanding and vector representations to find documents that are conceptually related to the query.",
      "source": "semantic-search.pdf"
    },
    {
      "id": "doc-027",
      "text": "Document preprocessing includes steps like lowercasing, tokenization, stop word removal, and stemming. These transformations normalize text and reduce dimensionality while preserving semantic content.",
      "source": "preprocessing.pdf"
    },
    {
      "id": "doc-028",
      "text": "BM25 has two main parameters: k1 controls term frequency saturation (typical values 1.2-2.0), and b controls document length normalization (typical values 0.75). These parameters can be tuned for specific datasets.",
      "source": "bm25-tuning.pdf"
    },
    {
      "id": "doc-029",
      "text": "Zero-shot learning enables models to perform tasks without task-specific training data. Large language models demonstrate impressive zero-shot capabilities across various NLP tasks including classification, summarization, and question answering.",
      "source": "zero-shot.pdf"
    },
    {
      "id": "doc-030",
      "text": "Context windows in language models define the maximum amount of text the model can process at once. GPT-4 supports context windows up to 128K tokens, enabling processing of long documents and extensive conversation history.",
      "source": "context-windows.pdf"
    },
    {
      "id": "doc-031",
      "text": "Hallucination in language models refers to generating plausible-sounding but factually incorrect or nonsensical information. RAG systems mitigate hallucination by grounding responses in retrieved factual documents.",
      "source": "hallucination.pdf"
    },
    {
      "id": "doc-032",
      "text": "Prompt engineering involves crafting input prompts to elicit desired behaviors from language models. Effective prompts provide clear instructions, relevant context, and examples to guide the model's generation.",
      "source": "prompt-engineering.pdf"
    },
    {
      "id": "doc-033",
      "text": "Vector databases are specialized storage systems optimized for similarity search on high-dimensional vectors. They support operations like approximate nearest neighbor search, filtering, and real-time updates.",
      "source": "vector-databases.pdf"
    },
    {
      "id": "doc-034",
      "text": "Fine-tuning adapts a pre-trained model to specific tasks or domains by training on task-specific data. In RAG systems, fine-tuning can improve embedding quality for domain-specific retrieval.",
      "source": "fine-tuning.pdf"
    },
    {
      "id": "doc-035",
      "text": "Attention mechanisms allow neural networks to focus on relevant parts of the input when generating output. Transformers use multi-head self-attention to capture relationships between all positions in a sequence.",
      "source": "attention.pdf"
    },
    {
      "id": "doc-036",
      "text": "Batch processing in RAG systems allows efficient generation of embeddings for multiple documents simultaneously. Batching reduces overhead and improves throughput by leveraging GPU parallelism.",
      "source": "batch-processing.pdf"
    },
    {
      "id": "doc-037",
      "text": "Cold start problems in RAG systems occur when there's insufficient data to build effective indices. Solutions include using pre-trained models, synthetic data generation, and gradual index building.",
      "source": "cold-start.pdf"
    },
    {
      "id": "doc-038",
      "text": "Index optimization techniques for vector search include dimension reduction, quantization, and pruning. These methods trade some accuracy for significantly improved memory usage and search speed.",
      "source": "index-optimization.pdf"
    },
    {
      "id": "doc-039",
      "text": "Negative sampling is a training technique that includes negative examples to help models learn discriminative features. In dense retrieval, hard negatives (similar but irrelevant documents) are particularly valuable.",
      "source": "negative-sampling.pdf"
    },
    {
      "id": "doc-040",
      "text": "Query classification categorizes user queries by intent, complexity, or domain. RAG systems can route queries to different retrieval strategies based on classification results.",
      "source": "query-classification.pdf"
    },
    {
      "id": "doc-041",
      "text": "Caching strategies in RAG systems store frequently requested embeddings or search results to reduce latency. LRU (Least Recently Used) and TTL (Time To Live) policies manage cache invalidation.",
      "source": "caching.pdf"
    },
    {
      "id": "doc-042",
      "text": "Evaluation metrics for RAG systems include retrieval quality (precision, recall, MRR), generation quality (BLEU, ROUGE, perplexity), and end-to-end metrics like answer accuracy and user satisfaction.",
      "source": "rag-evaluation.pdf"
    },
    {
      "id": "doc-043",
      "text": "Sparse retrieval methods like BM25 excel at exact keyword matching and are interpretable through highlighted terms. They work well for technical queries, product names, and codes where exact matches are important.",
      "source": "sparse-retrieval.pdf"
    },
    {
      "id": "doc-044",
      "text": "Domain adaptation in embedding models involves fine-tuning on domain-specific data to improve retrieval accuracy. Legal, medical, and scientific domains often benefit from specialized embeddings.",
      "source": "domain-adaptation.pdf"
    },
    {
      "id": "doc-045",
      "text": "Approximate nearest neighbor (ANN) search trades perfect accuracy for speed by finding neighbors that are close enough rather than exactly the closest. Methods like HNSW and IVF enable sub-linear search time.",
      "source": "ann-search.pdf"
    },
    {
      "id": "doc-046",
      "text": "Token limits in language models constrain the length of input and output. Strategies for handling long documents include chunking, summarization, and hierarchical processing.",
      "source": "token-limits.pdf"
    },
    {
      "id": "doc-047",
      "text": "A/B testing in RAG systems compares different retrieval strategies, ranking models, or prompts. Metrics include user engagement, answer accuracy, and latency.",
      "source": "ab-testing.pdf"
    },
    {
      "id": "doc-048",
      "text": "Monitoring and observability in production RAG systems track metrics like retrieval latency, embedding generation time, cache hit rates, and error rates. Logging query patterns helps identify performance bottlenecks.",
      "source": "monitoring.pdf"
    },
    {
      "id": "doc-049",
      "text": "Cross-lingual retrieval enables searching documents in one language using queries in another. Multilingual embedding models like mBERT and XLM-R enable cross-lingual semantic search.",
      "source": "cross-lingual.pdf"
    },
    {
      "id": "doc-050",
      "text": "Explainability in RAG systems involves showing users which documents were used to generate answers and highlighting relevant passages. This builds trust and allows users to verify information sources.",
      "source": "explainability.pdf"
    }
  ],
  "queries": [
    {
      "id": "q-001",
      "text": "What is machine learning?",
      "relevantDocIds": ["doc-001"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-002",
      "text": "How does natural language processing work?",
      "relevantDocIds": ["doc-002"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-003",
      "text": "What is RAG in AI systems?",
      "relevantDocIds": ["doc-003"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-004",
      "text": "Explain vector embeddings",
      "relevantDocIds": ["doc-004", "doc-020"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-005",
      "text": "What is BM25 algorithm?",
      "relevantDocIds": ["doc-005", "doc-028"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-006",
      "text": "What is Elasticsearch used for?",
      "relevantDocIds": ["doc-006", "doc-018"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-007",
      "text": "What is Qdrant?",
      "relevantDocIds": ["doc-007"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-008",
      "text": "How does dense retrieval differ from keyword search?",
      "relevantDocIds": ["doc-008", "doc-005"],
      "queryType": "ImplicitFact"
    },
    {
      "id": "q-009",
      "text": "What is hybrid search?",
      "relevantDocIds": ["doc-009", "doc-012"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-010",
      "text": "What are sentence transformers?",
      "relevantDocIds": ["doc-010"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-011",
      "text": "Why is document chunking important?",
      "relevantDocIds": ["doc-011"],
      "queryType": "InterpretableRationale"
    },
    {
      "id": "q-012",
      "text": "What is Reciprocal Rank Fusion?",
      "relevantDocIds": ["doc-012"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-013",
      "text": "What are large language models?",
      "relevantDocIds": ["doc-013"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-014",
      "text": "How is precision calculated in IR?",
      "relevantDocIds": ["doc-014"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-015",
      "text": "How is recall measured?",
      "relevantDocIds": ["doc-015"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-016",
      "text": "What does Mean Reciprocal Rank measure?",
      "relevantDocIds": ["doc-016"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-017",
      "text": "Explain multi-tenancy in software",
      "relevantDocIds": ["doc-017"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-018",
      "text": "How do inverted indices work?",
      "relevantDocIds": ["doc-018", "doc-006"],
      "queryType": "ImplicitFact"
    },
    {
      "id": "q-019",
      "text": "What is HNSW algorithm?",
      "relevantDocIds": ["doc-019", "doc-007"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-020",
      "text": "What is OpenAI text-embedding-ada-002?",
      "relevantDocIds": ["doc-020"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-021",
      "text": "Explain TF-IDF weighting",
      "relevantDocIds": ["doc-021"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-022",
      "text": "How is cosine similarity calculated?",
      "relevantDocIds": ["doc-022"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-023",
      "text": "What is query expansion?",
      "relevantDocIds": ["doc-023"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-024",
      "text": "How do re-ranking models work?",
      "relevantDocIds": ["doc-024"],
      "queryType": "ImplicitFact"
    },
    {
      "id": "q-025",
      "text": "What is knowledge distillation?",
      "relevantDocIds": ["doc-025"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-026",
      "text": "What is semantic search?",
      "relevantDocIds": ["doc-026", "doc-008"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-027",
      "text": "What are document preprocessing steps?",
      "relevantDocIds": ["doc-027"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-028",
      "text": "What are the BM25 parameters k1 and b?",
      "relevantDocIds": ["doc-028"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-029",
      "text": "What is zero-shot learning?",
      "relevantDocIds": ["doc-029"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-030",
      "text": "What are context windows in LLMs?",
      "relevantDocIds": ["doc-030"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-031",
      "text": "How does RAG prevent hallucination?",
      "relevantDocIds": ["doc-031", "doc-003"],
      "queryType": "InterpretableRationale"
    },
    {
      "id": "q-032",
      "text": "What is prompt engineering?",
      "relevantDocIds": ["doc-032"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-033",
      "text": "What are vector databases?",
      "relevantDocIds": ["doc-033", "doc-007"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-034",
      "text": "What is fine-tuning in ML?",
      "relevantDocIds": ["doc-034"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-035",
      "text": "How do attention mechanisms work?",
      "relevantDocIds": ["doc-035"],
      "queryType": "ImplicitFact"
    },
    {
      "id": "q-036",
      "text": "Why use batch processing for embeddings?",
      "relevantDocIds": ["doc-036"],
      "queryType": "InterpretableRationale"
    },
    {
      "id": "q-037",
      "text": "What are cold start problems in RAG?",
      "relevantDocIds": ["doc-037"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-038",
      "text": "How can vector indices be optimized?",
      "relevantDocIds": ["doc-038"],
      "queryType": "ImplicitFact"
    },
    {
      "id": "q-039",
      "text": "What is negative sampling in training?",
      "relevantDocIds": ["doc-039"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-040",
      "text": "What is query classification?",
      "relevantDocIds": ["doc-040"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-041",
      "text": "What caching strategies work for RAG?",
      "relevantDocIds": ["doc-041"],
      "queryType": "ImplicitFact"
    },
    {
      "id": "q-042",
      "text": "How do you evaluate RAG systems?",
      "relevantDocIds": ["doc-042", "doc-014", "doc-015", "doc-016"],
      "queryType": "ImplicitFact"
    },
    {
      "id": "q-043",
      "text": "When should you use sparse retrieval?",
      "relevantDocIds": ["doc-043", "doc-005"],
      "queryType": "HiddenRationale"
    },
    {
      "id": "q-044",
      "text": "What is domain adaptation for embeddings?",
      "relevantDocIds": ["doc-044"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-045",
      "text": "What is approximate nearest neighbor search?",
      "relevantDocIds": ["doc-045", "doc-019"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-046",
      "text": "How do you handle long documents in LLMs?",
      "relevantDocIds": ["doc-046", "doc-011"],
      "queryType": "HiddenRationale"
    },
    {
      "id": "q-047",
      "text": "How do you A/B test RAG systems?",
      "relevantDocIds": ["doc-047"],
      "queryType": "ImplicitFact"
    },
    {
      "id": "q-048",
      "text": "What metrics should RAG systems monitor?",
      "relevantDocIds": ["doc-048"],
      "queryType": "ImplicitFact"
    },
    {
      "id": "q-049",
      "text": "What is cross-lingual retrieval?",
      "relevantDocIds": ["doc-049"],
      "queryType": "ExplicitFact"
    },
    {
      "id": "q-050",
      "text": "Why is explainability important in RAG?",
      "relevantDocIds": ["doc-050"],
      "queryType": "InterpretableRationale"
    },
    {
      "id": "q-051",
      "text": "How do AI systems learn patterns from data?",
      "relevantDocIds": ["doc-001"],
      "queryType": "ImplicitFact"
    },
    {
      "id": "q-052",
      "text": "Which search method is best for exact keyword matching?",
      "relevantDocIds": ["doc-005", "doc-043"],
      "queryType": "HiddenRationale"
    },
    {
      "id": "q-053",
      "text": "How can you combine multiple retrieval methods?",
      "relevantDocIds": ["doc-009", "doc-012"],
      "queryType": "ImplicitFact"
    },
    {
      "id": "q-054",
      "text": "What makes semantic search better than keyword search?",
      "relevantDocIds": ["doc-026", "doc-008"],
      "queryType": "HiddenRationale"
    },
    {
      "id": "q-055",
      "text": "How can retrieval systems understand query intent?",
      "relevantDocIds": ["doc-026", "doc-040"],
      "queryType": "HiddenRationale"
    }
  ]
}
